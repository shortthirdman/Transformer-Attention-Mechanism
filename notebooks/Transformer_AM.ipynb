{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d56db1-cd07-4450-924e-224dc501f467",
   "metadata": {},
   "source": [
    "### [Diving Into the Transformer Attention Mechanism](https://medium.com/data-science-collective/understanding-transformer-attention-mechanism-ffed36e821bb)\n",
    "\n",
    "The attention mechanism is essentially a set of matrix operations that form the mathematical foundation of the model.\n",
    "\n",
    "There are two scenarios where building from scratch makes sense: either to customize the architecture or for educational purposes.\n",
    "\n",
    "In custom setups, you’re required to modify the model’s mathematical and statistical core — a process that is both advanced and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ded653-8d9e-4e07-a19c-8b473a2ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2ff4d-6906-4194-9626-435d87a72711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.a Install PyTorch and NumPy\n",
    "!pip install -q -U torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee08e3-557b-416f-ae30-fe5be9275948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.b Imports\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf8925-0566-4671-b9f6-fd04f5523c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transformer model\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    # 2.a Constructor method\n",
    "    def __init__(self, vocab_size, embedding_dim,\n",
    "                 n_heads, n_layers, dropout):\n",
    "        \n",
    "        # 2.b Initialize parent class (nn.Module)\n",
    "        super().__init__()\n",
    "\n",
    "        # 2.c Store model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 2.d Embedding layer to map tokens to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # 2.e Multi-head self-attention layer\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 2.f Feed-forward network applied after attention\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # 2.g Final linear layer projecting back to vocab size\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # 2.h Forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 2.i Apply embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2.j Apply multi-head self-attention\n",
    "        x, _ = self.attention(x, x, x)\n",
    "\n",
    "        # 2.k Apply feed-forward network\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # 2.l Apply output projection\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aeb45-43d6-468c-abfd-a45e85dbb676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.a List all submodules\n",
    "model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058d318-99c3-408f-9429-c896216792f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.b Access the self-attention layer\n",
    "model.attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6deeac-7065-4c8f-8a41-a22c4636a813",
   "metadata": {},
   "source": [
    "#### Building a Transformer Model (Without Framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482da00-3846-46a0-a618-fd882f20c495",
   "metadata": {},
   "source": [
    "Build a Transformer model from scratch, without relying on any frameworks.\n",
    "\n",
    "The model will process sequences of 10 tokens — this is the first crucial definition for any Transformer-like architecture.\n",
    "\n",
    "The model's goal varies by application:\n",
    "\n",
    "- In computer vision, it might classify images.\n",
    "- In time series, it predicts future values.\n",
    "- In NLP, it generates output sequences from input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61835e61-7c74-4b14-b134-37f46326aff1",
   "metadata": {},
   "source": [
    "The _Transformer_ consists of four main components:\n",
    "\n",
    "- an **_embedding layer_** (converts words into vectors)\n",
    "- an **_attention mechanism_** (focuses on relevant parts of the sequence)\n",
    "- **_encoder/decoder blocks_** (for sequence processing)\n",
    "- a **_linear layer followed by softmax_** (to produce predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f945686-4b26-4be8-bb5b-4910eaf69b21",
   "metadata": {},
   "source": [
    "Implementing the simplest version of a Transformer — a **_Minimum Viable Product (MVP)_** — containing only the essential elements: `embedding`, `attention`, and `output`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be199c5-71e5-4e87-be70-61a47f739bb1",
   "metadata": {},
   "source": [
    "#### Model Construction — Initial Hyperparameters\n",
    "\n",
    "The first step is to define the model’s initial hyperparameters. These values should be chosen based on your specific objective. Start with some baseline values and adjust them as needed to achieve the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc3cd9-a489-4e2b-a0b9-6f7fdf627ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Imports\n",
    "import numpy as np\n",
    "\n",
    "# 4.a Model dimension\n",
    "dim_model = 64\n",
    "\n",
    "# 4.b Sequence length\n",
    "seq_length = 10\n",
    "\n",
    "# 4.c Vocabulary size\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de727691-e12e-4d9d-856c-75bd9185f176",
   "metadata": {},
   "source": [
    "#### Model Construction — Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec3252-5f6f-4047-9e22-0fdf9c0018c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Creates an embedding matrix for input tokens\n",
    "def embedding(input, vocab_size, dim_model):\n",
    "\n",
    "# 5.a Initialize embedding matrix with random values\n",
    "    # Each row corresponds to a token vector in the vocabulary\n",
    "    embed = np.random.randn(vocab_size, dim_model)\n",
    "   \n",
    " # 5.b For each token index in input, retrieve its embedding\n",
    "    return np.array([embed[i] for i in input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef26348-056b-4093-b4d7-019aaf0e0200",
   "metadata": {},
   "source": [
    "#### Q, K, and V Components in the Transformer\n",
    "The Transformer’s attention mechanism relies on three core components:\n",
    "**_Query (Q)_**, **_Key (K)_**, and **_Value (V)_**.\n",
    "These are used to process sequences and determine the relevance of each input element.\n",
    "\n",
    "- **Q (Query)**\n",
    "\n",
    "_Purpose_: Represents the current item of interest\n",
    "\n",
    "_Use_: Generates a query for each position in the sequence to assess relevance\n",
    "\n",
    "- **K (Key)**\n",
    "\n",
    "_Purpose_: Scores each element in the input\n",
    "\n",
    "_Use_: Compared against queries to determine attention weights\n",
    "\n",
    "- **V (Value)**\n",
    "\n",
    "_Purpose_: Contains the actual content to be extracted\n",
    "\n",
    "_Use_: Supplies the output, weighted by attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf33f1c-aad3-4448-a8c4-2c6970ea100c",
   "metadata": {},
   "source": [
    "#### Model Construction — Attention Mechanism\n",
    "The next component is the Attention Mechanism, which relies on three main elements: `Q`, `K`, and `V`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fa434-d82a-48bd-8cc0-c1876cf9dcdd",
   "metadata": {},
   "source": [
    "#### Model Construction — Softmax Activation Function\n",
    "The _Softmax_ function is a classic activation used in neural networks, especially for classification tasks. It transforms raw output values (logits) into probabilities that sum to 1, with each value ranging between 0 and 1.\n",
    "\n",
    "When the model produces logits as part of its computation, Softmax converts them into interpretable probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f16a1-91c4-4fa8-a7c0-f2f240465059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Softmax activation function\n",
    "def softmax(x):\n",
    "\n",
    "# 6.a Compute exponentials adjusted by max value to avoid overflow\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "# 6.b Normalize across the last axis (axis=-1)\n",
    "    # Reshape ensures correct broadcasting in multidimensional inputs\n",
    "    return e_x / e_x.sum(axis=-1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bf59b-c656-4412-bce8-54331b831ff2",
   "metadata": {},
   "source": [
    "#### Model Construction — Scaled Dot-Product\n",
    "\n",
    "The `scaled_dot_product_attention` function is a core component of the attention mechanism in Transformer models.\n",
    "\n",
    "It computes attention scores between `queries`, `keys`, and `values`, allowing the model to assign different levels of importance to each part of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac34e93-e839-4adb-8ac5-867dd16b8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Scaled dot-product attention function\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "\n",
    "# 7.a Compute dot product between Q and K transposed\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "\n",
    "# 7.b Get the dimensionality of the key vectors\n",
    "    depth = K.shape[-1]\n",
    "\n",
    "# 7.c Scale the logits by the square root of depth\n",
    "    logits = matmul_qk / np.sqrt(depth)\n",
    "\n",
    "# 7.d Apply softmax to get attention weights\n",
    "    attention_weights = softmax(logits)\n",
    "\n",
    "# 7.e Compute weighted sum of values\n",
    "    output = np.dot(attention_weights, V)\n",
    "\n",
    "# 7.f Return final output\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa29d2-1cc8-48be-ad95-d536d16a4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Final model function\n",
    "def transformer_model(input):\n",
    "\n",
    "    # 8.a Embedding layer\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "    \n",
    "    # 8.b Scaled dot-product attention\n",
    "    attention_output = scaled_dot_product_attention(\n",
    "        embedded_input, embedded_input, embedded_input\n",
    "    )\n",
    "    \n",
    "    # 8.c Linear layer followed by softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "    \n",
    "    # 8.d Select highest probability indices\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc51c3-d9b9-4830-ad09-4842e9087166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Linear transformation followed by softmax\n",
    "def linear_and_softmax(input):\n",
    "\n",
    "# 9.a Randomly initialize weight matrix\n",
    "# Maps from model dimension to vocabulary size\n",
    "    W = np.random.randn(dim_model, vocab_size)\n",
    "    \n",
    "# 9.b Apply linear transformation (dot product)\n",
    "# Projects input into vocabulary space\n",
    "    logits = np.dot(input, W)\n",
    "    \n",
    "# 9.c Convert logits into probabilities\n",
    "    return softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780d2c4-4b4d-4ccb-85c9-3a07ea7d0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Final transformer model function\n",
    "def transformer_model(input):\n",
    "\n",
    "# 10.a Generate embedding for input sequence\n",
    "    embedded_input = embedding(input, vocab_size, dim_model)\n",
    "    \n",
    "# 10.b Apply attention mechanism using Q, K, V = embedded_input\n",
    "    attention_output = scaled_dot_product_attention(\n",
    "        embedded_input, embedded_input, embedded_input\n",
    "    )\n",
    "    \n",
    "# 10.c Apply linear transformation followed by softmax\n",
    "    output_probabilities = linear_and_softmax(attention_output)\n",
    "    \n",
    "# 10.d Select token indices with highest probabilities\n",
    "    output_indices = np.argmax(output_probabilities, axis=-1)\n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711da34a-ddab-460f-97b0-cf1030f489c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Generate random input sequence for inference\n",
    "input_sequence = np.random.randint(0, vocab_size, seq_length)\n",
    "\n",
    "print(\"Input Sequence:\", input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c4cb2-3638-4402-acc7-b63d43314ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.a Predict output using transformer model\n",
    "output = transformer_model(input_sequence)\n",
    "\n",
    "print(\"Model Output:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
